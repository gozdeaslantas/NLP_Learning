{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1x74mfIxm1JP",
    "outputId": "77c77c91-ad91-4587-eb63-3420bbd83936"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rn\n",
    "import copy\n",
    "import string\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score,cross_validate\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jy5aac1EmmW0"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train_tweets.csv', names=['Tweets', 'Sentiment'])\n",
    "val_df = pd.read_csv('data/test_tweets.csv', names=['Tweets', 'Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tuj5-JurmoWo",
    "outputId": "0481f760-6f1b-4438-ec55-5eaaa4cdc58a"
   },
   "outputs": [],
   "source": [
    "train_df.replace({\"olumsuz\": \"Negative\", \"olumlu\": \"Positive\", \"notr\": \"Neutral\"}, inplace=True)\n",
    "print('Train Sentiment classes', train_df['Sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-d06fXRimqKG",
    "outputId": "8eed3d17-cea5-4a4e-953a-a330b9b857ba"
   },
   "outputs": [],
   "source": [
    "val_df.replace({\"olumsuz\": \"Negative\", \"olumlu\": \"Positive\", \"notr\": \"Neutral\"}, inplace=True)\n",
    "print('Valid Sentiment classes', train_df['Sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BlsvadVumsP5",
    "outputId": "c1612110-c581-43fa-a420-de79217f1c86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    5511\n",
       "Neutral     4658\n",
       "Positive    3663\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('Sentiment').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sAAAX50Zmt3D",
    "outputId": "00432991-bd82-4d3e-f6b9-d558433bf22d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Negative    1377\n",
       "Neutral     1164\n",
       "Positive     916\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.groupby('Sentiment').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "C9qzrzPwvPwX"
   },
   "outputs": [],
   "source": [
    "#@markdown This method intends to clean the given text \\\n",
    "#@markdown from graphical emojis, textual emojis, \n",
    "#@markdown mentions, and urls\n",
    "import emoji\n",
    "import re \n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "textual_emoji_patterns = r\"\"\"(\\:\\)|\\:\\(|<3|\\:\\/|\\:-\\/|\\:\\||\\:p|\\(\\:|\\:\\D)\"\"\"\n",
    "\n",
    "mention_patterns = r\"\"\"(?:@[\\w_]+)\"\"\"\n",
    "\n",
    "url_patterns = r'http\\//t.co\\/[^\\s]+|http\\S+|www\\S+'\n",
    "\n",
    "multiple_spaces = r' {2,}'\n",
    "\n",
    "tr_stopwords = stopwords.words('turkish')\n",
    "\n",
    "def remove_textual_emoji(text):\n",
    "    return re.sub(textual_emoji_patterns, '', text)\n",
    "\n",
    "def remove_graphical_emoji(text): \n",
    "    return emoji.replace_emoji(text, '')\n",
    "\n",
    "def remove_mentions(text):\n",
    "    return re.sub(mention_patterns, '', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(url_patterns, '', text)\n",
    "\n",
    "def remove_multiple_spaces(text):\n",
    "    return re.sub(multiple_spaces, '', text)\n",
    "\n",
    "cleaned_text = []\n",
    "def clean_tweets(text):\n",
    "    text = remove_textual_emoji(text)\n",
    "    text = remove_graphical_emoji(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_multiple_spaces(text)\n",
    "    text = text.rstrip()\n",
    "    text = text.lower()\n",
    "    stop_words = set(tr_stopwords)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    \n",
    "    cleaned_text.append(\" \".join([w for w in words]))\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "id": "Xjk5QlfG_nZ2",
    "outputId": "067ec8ca-10b1-4666-f246-96a769f5f105"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Ulan Wifi'ye bağlıyım ben. Ona bağlıyken Turkc...\n",
       "1    20 dk 1 GB internet 500 mb sadece kaşar turkce...\n",
       "2    Ayrıca turkcell superonline reklamı kadar da k...\n",
       "3                               Turkcell çok pahalı ya\n",
       "4                  Turkcell Kaş'ta internetin cekmiyor\n",
       "5    Turkcell'in Allah belası versin demek isterdim...\n",
       "6                             Canın cehenneme turkcell\n",
       "7    Turkcell yönetimini eline geçiren AKP hükümeti...\n",
       "8    Turkcell şerefsizdir aksini iddia eden turkcel...\n",
       "9         Turkcell'den elimi ayağımı çektiğim iyi oldu\n",
       "Name: Tweets, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train_df[\"Tweets\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iHfq21pAyYWQ",
    "outputId": "f182b3cf-2df8-462e-aeae-dce72a311c17"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean the tweets:  88%|████████▊ | 12109/13832 [01:21<00:18, 94.51it/s]"
     ]
    }
   ],
   "source": [
    "clean_tweet = []\n",
    "for row in tqdm(train_df.itertuples(), desc=\"Clean the tweets\", total=len(train_df)):\n",
    "    doc = f\"{clean_tweets(row.Tweets)}\"\n",
    "    clean_tweet.append(doc)\n",
    "train_df[\"Tweets\"] = clean_tweet\n",
    "train_df[\"Tweets\"].head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "[TR]Sentiment_Analysis_with_NB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
