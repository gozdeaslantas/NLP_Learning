{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gIBJb89nbU0"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install transformers[sentencepiece]\n",
        "from google.colab import output\n",
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import list_models, HfApi, ModelFilter\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "notebook_login()"
      ],
      "metadata": {
        "id": "vdOutLKpyYzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_models()"
      ],
      "metadata": {
        "id": "sRUNbHjHyiMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_api = HfApi()\n",
        "models = hf_api.list_models(\n",
        "    filter=ModelFilter(\n",
        "        task=[\"text-generation\", \"conversational\"]\n",
        "    )\n",
        ")\n",
        "\n",
        "model_Ids = [m.modelId for m in models]"
      ],
      "metadata": {
        "id": "YZN2_7-7zE5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "model_picker = widgets.Dropdown(options=model_Ids, value='microsoft/DialoGPT-medium')"
      ],
      "metadata": {
        "id": "qPgoWDOa0rWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = model_picker.value\n",
        "print(f\"Selected model is: {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "HhrTpxtqndGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "step = 0\n",
        "user_input = input(\">> User:\")\n",
        "while user_input.lower()!='exit':\n",
        "    \n",
        "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
        "    new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "    # append the new user input tokens to the chat history\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "\n",
        "    # generated a response while limiting the total chat history to 1000 tokens, \n",
        "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "    # pretty print last ouput tokens from bot\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "    step = step + 1\n",
        "    user_input = input(\">> User:\")"
      ],
      "metadata": {
        "id": "i723lgqwnelK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}